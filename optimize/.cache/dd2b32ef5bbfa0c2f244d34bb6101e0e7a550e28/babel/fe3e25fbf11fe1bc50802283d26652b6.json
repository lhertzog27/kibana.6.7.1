{"remainingRequest":"/home/anthony/git_workspaces/kibana/node_modules/thread-loader/dist/cjs.js??ref--9-1!/home/anthony/git_workspaces/kibana/node_modules/babel-loader/lib/index.js??ref--9-2!/home/anthony/git_workspaces/kibana/src/ui/public/agg_response/tabify/tabify.js","dependencies":[{"path":"/home/anthony/git_workspaces/kibana/src/ui/public/agg_response/tabify/tabify.js","mtime":1567631711694},{"path":"/home/anthony/git_workspaces/kibana/node_modules/cache-loader/dist/cjs.js","mtime":1567666236302},{"path":"/home/anthony/git_workspaces/kibana/node_modules/thread-loader/dist/cjs.js","mtime":1567666239443},{"path":"/home/anthony/git_workspaces/kibana/node_modules/babel-loader/lib/index.js","mtime":1567666227676}],"contextDependencies":[],"result":["'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.tabifyAggResponse = tabifyAggResponse;\n\nvar _lodash = require('lodash');\n\nvar _lodash2 = _interopRequireDefault(_lodash);\n\nvar _response_writer = require('./_response_writer');\n\nvar _buckets = require('./_buckets');\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\n/**\n * Sets up the ResponseWriter and kicks off bucket collection.\n *\n * @param {AggConfigs} aggs - the agg configs object to which the aggregation response correlates\n * @param {Object} esResponse - response that came back from Elasticsearch\n * @param {Object} respOpts - options object for the ResponseWriter with params set by Courier\n * @param {boolean} respOpts.minimalColumns - setting to true will only return a column for the last bucket/metric instead of one for each level\n * @param {boolean} respOpts.partialRows - vis.params.showPartialRows: determines whether to return rows with incomplete data\n * @param {Object} respOpts.timeRange - time range object, if provided\n */\nfunction tabifyAggResponse(aggs, esResponse) {\n  var respOpts = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n\n  var write = new _response_writer.TabbedAggResponseWriter(aggs, respOpts);\n\n  var topLevelBucket = _lodash2.default.assign({}, esResponse.aggregations, {\n    doc_count: esResponse.hits.total\n  });\n\n  collectBucket(write, topLevelBucket, '', 1);\n\n  return write.response();\n}\n\n/**\n * read an aggregation from a bucket, which *might* be found at key (if\n * the response came in object form), and will recurse down the aggregation\n * tree and will pass the read values to the ResponseWriter.\n *\n * @param {object} bucket - a bucket from the aggResponse\n * @param {undefined|string} key - the key where the bucket was found\n * @returns {undefined}\n */\n/*\n * Licensed to Elasticsearch B.V. under one or more contributor\n * license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright\n * ownership. Elasticsearch B.V. licenses this file to you under\n * the Apache License, Version 2.0 (the \"License\"); you may\n * not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\nfunction collectBucket(write, bucket, key, aggScale) {\n  var column = write.aggStack.shift();\n  var agg = column.aggConfig;\n  var aggInfo = agg.write(write.aggs);\n  aggScale *= aggInfo.metricScale || 1;\n\n  switch (agg.type.type) {\n    case 'buckets':\n      var buckets = new _buckets.TabifyBuckets(bucket[agg.id], agg.params, write.timeRange);\n      if (buckets.length) {\n        buckets.forEach(function (subBucket, key) {\n          // if the bucket doesn't have value don't add it to the row\n          // we don't want rows like: { column1: undefined, column2: 10 }\n          var bucketValue = agg.getKey(subBucket, key);\n          var hasBucketValue = typeof bucketValue !== 'undefined';\n          if (hasBucketValue) {\n            write.bucketBuffer.push({ id: column.id, value: bucketValue });\n          }\n          collectBucket(write, subBucket, agg.getKey(subBucket, key), aggScale);\n          if (hasBucketValue) {\n            write.bucketBuffer.pop();\n          }\n        });\n      } else if (write.partialRows) {\n        // we don't have any buckets, but we do have metrics at this\n        // level, then pass all the empty buckets and jump back in for\n        // the metrics.\n        write.aggStack.unshift(column);\n        passEmptyBuckets(write, bucket, key, aggScale);\n        write.aggStack.shift();\n      } else {\n        // we don't have any buckets, and we don't have isHierarchical\n        // data, so no metrics, just try to write the row\n        write.row();\n      }\n      break;\n    case 'metrics':\n      var value = agg.getValue(bucket);\n      // since the aggregation could be a non integer (such as a max date)\n      // only do the scaling calculation if it is needed.\n      if (aggScale !== 1) {\n        value *= aggScale;\n      }\n      write.metricBuffer.push({ id: column.id, value: value });\n\n      if (!write.aggStack.length) {\n        // row complete\n        write.row();\n      } else {\n        // process the next agg at this same level\n        collectBucket(write, bucket, key, aggScale);\n      }\n\n      write.metricBuffer.pop();\n\n      break;\n  }\n\n  write.aggStack.unshift(column);\n}\n\n// write empty values for each bucket agg, then write\n// the metrics from the initial bucket using collectBucket()\nfunction passEmptyBuckets(write, bucket, key, aggScale) {\n  var column = write.aggStack.shift();\n  var agg = column.aggConfig;\n\n  switch (agg.type.type) {\n    case 'metrics':\n      // pass control back to collectBucket()\n      write.aggStack.unshift(column);\n      collectBucket(write, bucket, key, aggScale);\n      return;\n\n    case 'buckets':\n      passEmptyBuckets(write, bucket, key, aggScale);\n  }\n\n  write.aggStack.unshift(column);\n}",null]}